---
title: enhance the new GCN, read EnGN
published_date: "2021-05-24 20:23:50 +0000"
layout: default.liquid
is_draft: false
--- 
# enhance the new GCN, read EnGN

- [x] read the HyGcn paper again to get the detail of the computation overhead of the control of tiles(windows)
- [x] read EnGN
- [ ] change the new-gcn

## menu

- [enhance the new GCN, read EnGN](#enhance-the-new-gcn-read-engn)
  - [menu](#menu)
  - [read the paper EnGN of the details](#read-the-paper-engn-of-the-details)
    - [conclusion](#conclusion)
    - [confusions:](#confusions)
    - [the issue and background](#the-issue-and-background)
    - [implementation](#implementation)
      - [ring edge reduce(RER)](#ring-edge-reducerer)
        - [hard to understand here. how it did it?](#hard-to-understand-here-how-it-did-it)
      - [Section5, proofing](#section5-proofing)
  - [code modifacations](#code-modifacations)

## read the paper EnGN of the details

### conclusion

main difference:
the ring PE model for both aggregation and MLP.
the new computational model for GNN
the different TILing scheduling.

### confusions:

1, this graph is wrong?? how it reduces it to 3, and what the graph mean, this is the most important contribution in this paper, but I can't understand it. [link go to here](#hard-to-understand-here-how-it-did-it)

### the issue and background

- issues
  - how to tailor a unified architecture that efficiently supports the diverse GNN model.
  - large graph, limited on-chip memory
  - power-law distribution create an imbalanced connection
- Solutions
  - ring-edge-reduce(RER)
  - graph property aware dataflow(GPA) that decouples the vertex property and the hardware structure. and scheduling of the stages.
  - graph tiling strategy. high degree of data reusability.
    - use either row-oriented and col-oriented(!)
  - 3 level cache, l2 is degree-aware vertex cache(DAVC)...
- GNN MODELS
![2](pics/2.png)
  - GCN
    - ![eq1](https://latex.codecogs.com/gif.latex?h^{l&plus;1}=ReLu({\widetilde{D}}^{-1/2}\widetilde{A}{\widetilde{D}}^{-1/2}h^lW^l),h^0=X)
    - 注意到，
![eq2](https://latex.codecogs.com/gif.latex?{\widetilde{D}}^{-1/2}\widetilde{A}{\widetilde{D}}^{-1/2})
里面，![eq2](https://latex.codecogs.com/gif.latex?{\widetilde{D}})是每个顶点的出度，而![eq2](https://latex.codecogs.com/gif.latex?{\widetilde{D}}^{-1/2}\widetilde{A}{\widetilde{D}}^{-1/2}) 对每条边，先出源节点的出度，再除目标结点的出度，最后得到了一个normalized Adj matrix.把结果×输入，NxN . Nxl = Nxl,对于结果中的每一行，
    - more explanation:<https://www.topbots.com/graph-convolutional-networks/>
    - ![Screenshot 2021-03-10 135603](/assets/Screenshot%202021-03-10%20135603.png)
  - GraphSage-Pool(GS-pool)
    - Deep walker:<https://medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72>
    - GraphSage:<https://towardsdatascience.com/an-intuitive-explanation-of-graphsage-6df9437ee64f>
  - Relational Graph convolutional network(R-GCN)
  - Gated graph convolution network(Gated-GCN)
  - Graph Recurrent network(GRN)
- motivation
  - aggregation is expensive, we may change the input and the output for aggregation????-->[section5](#section5-proofing)
  - hardware: HyGCN still fails to unleash the potential of the GNN acceleration:
    - HyGCN 只适用于小图
    - 20% vertices connected to 50-80% edges. but all vertices are evenly buffered!
    - HyGCN使用前后端，不同的部分可能使用不均匀，统一的硬件更下有效

### implementation

- EnGN implementation

- ![modle](/assets/Screenshot%202021-03-10%20125825.png)

In this figure, we can find the model has 3 stages.

---

![gcn](/assets/Screenshot%202021-03-10%20125841.png)

In this model, we can see GCN process MLP before agg

---

![ratio](/assets/Screenshot%202021-03-10%20125851.png)

In this figure, we can see. aggregate is the main bottleneck

---

![hrd](/assets/Screenshot%202021-03-10%20125909.png)

the overall arch

---

![det](/assets/Screenshot%202021-03-10%20125916.png)

#### ring edge reduce(RER)

the detailed. The aggregate procedure needs to collect the information according to  the edges information. In our design, the PE sends its data to the northern neighbors and receives the data sent from the southern neighbors for property aggregating. In this manner, a PE can select the relevant vertices to aggregate based on the control signal parsed from the edges during the data flow across the ring. (where the control flow from. what's the cost and implementation)

---

##### hard to understand here. how it did it?

![re](/assets/Screenshot%202021-03-10%20125929.png)

I cannot under stand this optimization. In the previouse introduction. each PE can only send the data to north. and here???

---

![det](/assets/Screenshot%202021-03-10%20125938.png)

the memory hierarchy for big nodes.

---

![split](/assets/Screenshot%202021-03-10%20125953.png)

the tiling algroithm... it's the same but introduce different scheduling mechanism. HyGCN only have row-major.

---

![io](/assets/Screenshot%202021-03-10%20130000.png)

Q=number of tiles,F=Q size. H, output size for a Q.

---

#### Section5, proofing

## code modifacations

support algorithm to choose the best input and output.

algorithm:

divide the input to many chunks. 